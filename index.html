<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>AIFAD by mmottl</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">AIFAD</h1>
      <h2 class="project-tagline">AIFAD - Automated Induction of Functions over Algebraic Data Types is a machine learning library and application written in OCaml which generalizes decision tree learning to algebraic data types.</h2>
      <a href="https://github.com/mmottl/aifad" class="btn">View on GitHub</a>
      <a href="https://github.com/mmottl/aifad/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/mmottl/aifad/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="aifad---automated-induction-of-functions-over-algebraic-data-types" class="anchor" href="#aifad---automated-induction-of-functions-over-algebraic-data-types" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>AIFAD - Automated Induction of Functions over Algebraic Data Types</h1>

<hr>

<h2>
<a id="what-is-aifad" class="anchor" href="#what-is-aifad" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What is AIFAD?</h2>

<p>AIFAD stands for <em>Automated Induction of Functions over Algebraic Data Types</em>
and is an application written in <a href="http://www.ocaml.org">OCaml</a> that improves
decision tree learning by supporting significantly more complex kinds of data.
This allows users to more conveniently describe the data they want to learn
functions on and can improve the accuracy and complexity of resulting models.</p>

<h2>
<a id="features" class="anchor" href="#features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Features</h2>

<ul>
<li><p>Handles multi-valued attributes.  This has already become widespread among
decision tree learners, but some implementations still only support
binary ones.</p></li>
<li><p>Allows multiple target attributes.  There are hardly any implementations
that support this feature in a unified way.</p></li>
<li><p>Supports structured data, both for input and target attributes.
Structured data is still a hot research topic in machine learning.</p></li>
<li><p>Allows recursive data.  AIFAD can learn models for recursive data but
only induce non-recursive functions.</p></li>
<li><p>Handles missing values by allowing users to encode them as option types.
This is a clean solution compared to often-employed hacks and has
competitive accuracy.</p></li>
<li><p>Provides several heuristics for generating models, including a generalized
gain ratio criterion as used by the state-of-the-art decision tree
learner C4.5.</p></li>
<li><p>Can read data specifications as understood by C4.5.</p></li>
<li><p>Pretty efficient.  Even though it was written to handle structured
data, it is only about 3-5 times slower than C4.5 on datasets that the
latter supports.  The memory footprint is somewhat larger due to the
impossibility of performing certain operations in-place, but still low
enough to handle even large datasets (hundreds of thousands of samples
with 10s of attributes) on stock hardware.</p></li>
</ul>

<p>AIFAD essentially offers you the same amount of expressiveness on both sides
of the data (input and output).  Discrete data specifications as supported
by more common decision tree learners (e.g. C4.5) are a strict subcase of
algebraic data types as used in AIFAD.  Furthermore, all kinds of algorithms
that operate on "normal" decision trees (e.g. pruning algorithms) should be
generalizable to the more expressive representation.</p>

<h3>
<a id="missing-features" class="anchor" href="#missing-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Missing Features</h3>

<ul>
<li><p>Cannot (yet) handle numerical values.  Users can work around this by
preprocessing numeric data to cast it into discrete domains.  Since AIFAD
supports structured data, encodings more flexible than mere assignment to
"flat" domains can be used.  This will likely improve predictive accuracy.
If there is numerical data in C4.5-data, AIFAD will ignore it during
learning, thus only considering discrete data.</p></li>
<li><p>Does not (yet) provide post-pruning.  Can be easily added (e.g. reduced
error pruning).</p></li>
<li><p>Cannot (yet) learn recursive functions.</p></li>
</ul>

<hr>

<h2>
<a id="using-aifad" class="anchor" href="#using-aifad" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using AIFAD</h2>

<h3>
<a id="specification-of-algebraic-data-types" class="anchor" href="#specification-of-algebraic-data-types" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Specification of Algebraic Data Types</h3>

<p>Before AIFAD can learn from data, you will have to tell it what your data
looks like.  This requires creating a file with the extension <code>.ads</code>
(algebraic data type specification), which contains a set of (possibly
recursive) type equations.  If you happen to know modern functional or logic
programming languages, this concept will be common to you.</p>

<p>Here is an example of a specification file (file <code>examples/test.ads</code> in
the distribution):</p>

<pre lang="text"><code># This is a comment

diameter = Large | Small.

cheese = Mozzarella | Gorgonzola.

topping = Cheese cheese | Tomatoes | Mushrooms.

spiced = False | True.

meal =
  | WienerSchnitzel diameter
  | Pizza (diameter * spiced * topping)
  | TapirSoup spiced.

drink = Water | Beer | Wine.

satisfaction = Low | High | Very satisfaction.

domain = meal.
codomain = satisfaction.
</code></pre>

<p>The above equations contain the name of data types on their left-hand side
and their specification on the right-hand side.  For example, a value of
type <code>cheese</code> can be <code>Mozzarella</code> or <code>Gorgonzola</code>.  A (pizza) topping can
be <code>Cheese</code>, <code>Tomatoes</code>, or <code>Mushrooms</code>, but <code>Cheese</code> values require the
particular sort of cheese, e.g. <code>Cheese Gorgonzola</code>.</p>

<p>Some kinds of data can also be described in more detail by several other data
values.  For example, the <code>meal</code> value <code>Pizza</code> requires the specification of
<code>diameter</code>, <code>spiced</code>, and <code>topping</code>.  An example instance of this type of
value is:</p>

<pre lang="text"><code>Pizza (Large, False, Cheese Mozzarella)
</code></pre>

<p>The data type <code>satisfaction</code> demonstrates recursion.  A restaurant guest's
satisfaction might be <code>Low</code>, <code>High</code>, <code>Very High</code>, <code>Very (Very Low)</code>, etc.</p>

<p><code>domain</code> is a special typename and tells AIFAD what values should be used as
input to your problem, whereas <code>codomain</code> specifies the output.  This example
obviously sets up the specification of how to predict a restaurant guest's
satisfaction given the meal.</p>

<p>Note that if specifications do not make sense (e.g. if types cannot be reached,
violate liveness properties, etc.), then they are currently pruned down to the
largest set of type equations that is still reasonable.  AIFAD currently does
not yet provide good error messages to help users improve their specifications,
but for most applications this should not be much of a concern.</p>

<h4>
<a id="lexical-conventionslexical" class="anchor" href="#lexical-conventionslexical" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexical conventions[^lexical]</h4>

<p>The following characters are considered blanks: space, newline, horizontal
tabulation, carriage return, line feed and form feed.  Blanks are ignored,
but they separate adjacent identifiers, literals and keywords that would
otherwise be confused as one single identifier, literal or keyword.</p>

<p>Comments in specifications start with a hash <code>#</code> and are valid up to the
end of the current line.</p>

<p>Identifiers are sequences of letters, digits, _ (the underscore character),
and ' (the single quote), starting with a letter or an underscore.  Letters
contain at least the 52 lowercase and uppercase letters from the ASCII set.
The current implementation (except on MacOS) also recognizes as letters all
accented characters from the ISO 8859-1 (<code>ISO Latin 1</code>) set.  All characters
in an identifier are meaningful.  The current implementation places no limits
on the number of characters of an identifier.</p>

<p>Type names are identifiers, but start with either a lowercase character or
an underscore _.  Data constructors are identifiers, but always start with
uppercase characters.  The only keywords are <code>domain</code> and <code>codomain</code>.</p>

<h4>
<a id="syntax" class="anchor" href="#syntax" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Syntax</h4>

<p>Here is the EBNF-grammar of type definitions, of which you can have almost
arbitrarily many in your specification:</p>

<pre lang="text"><code>type-definition ::= type-name '=' rhs '.'
rhs             ::= sum { '|' sum }* | type
sum             ::= data-constructor | data-constructor type
type            ::= type-name | product
product         ::= '(' type { '*' type }+ ')'
</code></pre>

<p>Types can only be defined once.  Data constructors may appear only once at
each right-hand side, but may be reused in other definitions.  You will have
to name one of your types <code>domain</code>, which defines the shape of your input
data, and another type <code>codomain</code> to indicate the form of your output data.</p>

<h3>
<a id="data-files" class="anchor" href="#data-files" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data Files</h3>

<p>Data files take the extension <code>.add</code>.  Here is an example for a data file
used for learning models (file <code>examples/test.add</code> in the distribution):</p>

<pre lang="text"><code>WienerSchnitzel Large -&gt; High.
WienerSchnitzel Small -&gt; Low.
Pizza (Large, True, Cheese Mozzarella) -&gt; Very High.
Pizza (Large, False, Cheese Gorgonzola) -&gt; Very (Very High).
Pizza (Small, True, Cheese Mozzarella) -&gt; High.
Pizza (Small, False, Cheese Gorgonzola) -&gt; Very High.
TapirSoup False -&gt; Very Low.
TapirSoup True -&gt; Very Low.
TapirSoup True -&gt; Very High.
</code></pre>

<p>Such files follow the same lexical conventions as data specifications.</p>

<p>There are three kinds of data files:</p>

<ol>
<li>Files containing mappings from input values to output values used for
 training (see example above).</li>
<li>Data files containing only input values.</li>
<li>Data files containing only output values.</li>
</ol>

<p>Depending on the operation you want to perform, one or several of the above
formats are valid.</p>

<h4>
<a id="syntax-1" class="anchor" href="#syntax-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Syntax</h4>

<p>Here is the EBNF-grammar of samples as used in data files containing mappings
from input to output values:</p>

<pre lang="text"><code>sample        ::= data '-&gt;' data '.'
data          ::= sum-value | product-value
sum-value     ::= data-constructor | data-constructor argument
argument      ::= data-constructor
                | '(' data-constructor argument ')'
                | product-value
product-value ::= '(' data { ',' data }+ ')'
</code></pre>

<p>Files that contain only input or output data do not contain <code>samples</code> but
<code>data</code> followed by a dot.</p>

<h3>
<a id="c45-compatible-data" class="anchor" href="#c45-compatible-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>C4.5-compatible Data</h3>

<p>AIFAD can handle data in C4.5-format.  Data specifications of this kind
are stored in files having the extension <code>.names</code> and related data in files
having the extension <code>.data</code>.  You can find an example for this data format
in the files <code>c45.names</code> and <code>c45.data</code> in the <code>examples</code> directory in the
distribution.</p>

<p>The <code>.names</code>-file contains in its first line a comma-separated list of class
values terminated by a dot.  Each following line starts with the name of an
attribute followed by a colon and again a comma-separated list of attribute
values terminated by a dot.  Instead of this list, the keyword "continuous"
may be used to indicate numerical data.  C4.5 can also handle continuous
attributes, which are currently ignored by AIFAD.  This feature may be added
some time in the future.  AIFAD will, however, make use of all of the discrete
data available.</p>

<p>Data files in C4.5-format consist of comma-separated lists of data stored
linewise, which must match the order and contents of the attributes
specified in the corresponding <code>.names</code>-file.  The class values are
contained in the last column of these comma-separated lists.</p>

<h3>
<a id="learning-from-data" class="anchor" href="#learning-from-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Learning from Data</h3>

<p>Using AIFAD on the command-line, the learning mode is turned on by the
flag <code>-learn</code>.  In this case you will also have to provide a name for the
specification of the data.  This can be done in two ways:</p>

<ol>
<li><p>Using the <code>-spec</code> flag, which takes the filename of the specification
 but does not set a name for the data file (default: <code>stdin</code>).</p></li>
<li><p>Using the <code>-stem</code> flag, which expects a filestem and sets both the name
 of the specification and the data appropriately.  E.g. <code>-stem foo</code>
 would set a specification file <code>foo.ads</code> and a data file <code>foo.add</code>
 for standard AIFAD data.  When using C4.5 data, the names would be
 <code>foo.names</code> and <code>foo.data</code> respectively.  C4.5 mode can be indicated
 with the flag <code>-c45</code> on the command-line.</p></li>
</ol>

<h4>
<a id="examples" class="anchor" href="#examples" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h4>

<p>The following command learns standard AIFAD data, the specification being
in file <code>foo.ads</code> while the data is being read via <code>stdin</code> from file <code>bar.add</code>:</p>

<div class="highlight highlight-source-shell"><pre>aifad -learn -spec foo.ads <span class="pl-k">&lt;</span> bar.add</pre></div>

<p>The next command learns specification <code>foo.ads</code> and data <code>foo.add</code> given
a stem:</p>

<div class="highlight highlight-source-shell"><pre>aifad -learn -stem foo</pre></div>

<p>The following command learns specification <code>foo.names</code> and <code>foo.data</code> given
a stem (C4.5-format):</p>

<div class="highlight highlight-source-shell"><pre>aifad -c45 -learn -stem foo</pre></div>

<p>By default the system will learn a model and print it in a human-readable
format to <code>stdout</code>.  The model complexity, i.e. the number of bits consumed
from input data to output one bit of information, is printed to <code>stderr</code>.
If <code>-no-hmod</code> is specified, no human-readable model will be printed.</p>

<p>If you also want to store the learned model in a machine-readable format for
later use (application), then you will have to specify a name for the model
file using the <code>-model</code> flag, e.g.:</p>

<div class="highlight highlight-source-shell"><pre>aifad -c45 -learn -stem foo -model mymodel.c45model</pre></div>

<p>Note that C4.5-models and ones for standard AIFAD specifications are not
compatible.  AIFAD makes sure that you do not accidentally apply some model
to the wrong data.</p>

<h4>
<a id="models" class="anchor" href="#models" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Models</h4>

<p>Models are printed directly in the OCaml-language.  They are represented by a
function <code>model</code>, which maps the tuple (or single attribute) of input to the
corresponding output.  Variables that are unused are unnamed, i.e. written as
underscore <code>_</code>.  The data constructors are represented by so-called polymorphic
variants so that there is no need for an explicit data specification.
The human-readable model file should actually be perfectly valid OCaml.</p>

<p>Input data is matched in match-clauses so as to discriminate between
different cases.  As soon as the result is clear to the learning algorithm,
constructors of the output specification will be used to construct a result.
If the result is only partially known, let-bindings will factor out the
already predictable part, and more deeply nested matches predict the rest.</p>

<p>When no other actions are mentioned on the command-line except for the
machine-readable model, the model will be printed in human-readable form on
<code>stdout</code>, e.g.:</p>

<div class="highlight highlight-source-shell"><pre>aifad -model mymodel.c45model</pre></div>

<h3>
<a id="learning-parameters" class="anchor" href="#learning-parameters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Learning Parameters</h3>

<h4>
<a id="variable-selection" class="anchor" href="#variable-selection" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Variable selection</h4>

<p>Currently there is only one hardly different alternative to using the gain
ratio criterion, namely the one used by Quinlan in C4.5.  This criterion can be
turned on using the flag <code>-gain-c45</code>.  It incorporates an additional constraint
that selects also on basis of the unscaled gains.  See [^quinlan1986]
for details.</p>

<h4>
<a id="pre-pruning" class="anchor" href="#pre-pruning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pre-pruning</h4>

<p>There is a currently still rather crude but quite effective and efficient
pre-pruning technique, which is turned on with <code>-with-min-gr</code>.  It stops
growing decision-trees during learning in a branch when the number of bits
required to select a variable for splitting exceeds the number of bits gained
by this step.  This effectively sets a minimum gain ratio at each selection.</p>

<h4>
<a id="different-entropy-measures" class="anchor" href="#different-entropy-measures" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Different entropy measures</h4>

<p>You can choose <code>-indep-entropy</code> when you assume that tuples (including ones
in substructures) are independent of each other.  This is faster than the
default, which assumes dependence.</p>

<p>Choose <code>-shallow-entropy</code> if you want to compute the entropy without
descending into structures.  This is much faster than the default, which
does so, especially for deeply nested data structures.</p>

<h4>
<a id="most-frequent-values" class="anchor" href="#most-frequent-values" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Most frequent values</h4>

<p>When no more splitting is possible or wanted, the most frequent value has
to be computed from the available data.  By setting <code>-indep-most-freq</code>,
you can choose to assume independence of tuple elements in the data, the
default assumes dependence again.</p>

<h4>
<a id="splitting-null-branches" class="anchor" href="#splitting-null-branches" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Splitting null branches</h4>

<p>When splitting data depending on some variable, it can happen that one branch
of the tree does not have any data for further learning (i.e. it is a <em>null
branch</em>).  Normally, the most frequent value of the previous set is then taken
as leaf.  By setting <code>-split-null</code>, the learning process may continue using
the remaining variables and last dataset.  Sometimes this hardly makes any
difference in efficiency, sometimes it can lead to extremely long learning
times and excessive model size.</p>

<p>Though the learning time is generally polynomial in the size of the
dataset, this flag can make the time complexity almost exponential in the
beginning, even for flat data structures.  When using structured data, time
complexity is currently indeed exponential then, though this problem might
be solved in a future release using very complicated models that employ CPS
(continuation-passing style).</p>

<p>At the moment we do not have empirical evidence that this experimental feature
significantly improves learning, but there is still some experimentation
left to be done.</p>

<h4>
<a id="random-models" class="anchor" href="#random-models" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Random models</h4>

<p>There is an experimental feature which allows you to generate additional
random models that are constructed such that at each split a variable is
chosen from a distribution built from the gain ratios.  This can be turned
on with the flag <code>-n-rand-gain</code>, which expects the number of additional
random models to generate as argument.  Another parameter <code>-t-rand-gain</code>
can be used to set the total number of seconds (floating point) that may be
spent learning in this case.  Whichever limit is reached first (time limit
or number of models) will determine the end of learning.</p>

<p>The models are necessarily equally accurate in-sample, but they may differ
in complexity: the latter is taken as decision criterion on which model to
choose (minimum complexity).  This seems quite interesting from an empirical
point of view, but needs further investigation.</p>

<h4>
<a id="handling-missing-values-in-c45-data" class="anchor" href="#handling-missing-values-in-c45-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Handling missing values in C4.5-data</h4>

<p>Samples containing missing values in their codomain are ignored.  For input
data there are four strategies for handling missing values (indicated by
<code>?</code> in the data):</p>

<ol>
<li><p>Ignore samples containing missing values.  Turned on by <code>-mv-ignore</code>.
 This is recommended if you are absolutely sure that missing values
 cannot occur, because it improves efficiency of learning, model size
 and efficiency of application.</p></li>
<li><p>Predict the most probable (frequent) value when a missing-value is present
in a sample.  Turned on by <code>-mv-mprob</code>.  When learning such models,
the algebraic encoding of input data is either <code>None</code> or <code>Some data</code>,
where data is a tuple of all variables in the C4.5-specification.</p></li>
<li><p>Add another constructor to encode missing values ("flat representation").
 Turned on by <code>-mv-flat</code>.</p></li>
<li><p>Algebraically encode the data by lifting constructors such that they
 are either <code>None</code> if some value is missing in an attribute or <code>Some
 value</code> if it is present.  This is the default (<code>-mv-lift-all</code>).</p></li>
</ol>

<h4>
<a id="factorization-of-models" class="anchor" href="#factorization-of-models" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Factorization of models</h4>

<p>Models are always constructed in such a way that matches are only performed
when some information (also from substructures) is really needed.  It can,
however, happen that it is only determinable after learning some subtree that
some partial predictions can already be made before the match.  By default,
models will also be simplified in this respect by using let-bindings that
factor out common information from subbranches.  You can prohibit this
feature by passing <code>-no-factor</code>.  There is hardly any performance penalty
for factorization so you will usually be happy with the default.</p>

<h3>
<a id="applying-models" class="anchor" href="#applying-models" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Applying Models</h3>

<p>If you have a machine-readable model file, you can apply it to input data.
There is a certain degree of flexibility here: you can apply a model either
to samples, i.e. mappings from input to output data, or to input data only</p>

<ul>
<li>even mixed!  In the case of mappings the output part in the data will be
ignore.  The input is read from standard input, the resulting predictions of
the model application will be printed in the corresponding format (standard
AIFAD or C4.5) to <code>stdout</code> or to a file whose name is passed to the flag
<code>-pred</code>.  For example:</li>
</ul>

<div class="highlight highlight-source-shell"><pre>aifad -apply -model mymodel.c45model <span class="pl-k">&lt;</span> input.data -pred prediction.data</pre></div>

<p>There is no need here to specify the type of model (C4.5), because the latter
knows what it is.</p>

<h3>
<a id="evaluating-results" class="anchor" href="#evaluating-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evaluating Results</h3>

<p>To evaluate the accuracy of some model application, we compare its result
to reference data.  The file containing the reference data is specified by
<code>-eval</code>, the data to be evaluated is read from <code>stdin</code>.  You will also
have to supply a data specification by either using <code>-spec</code> or <code>-stem</code>.
The result of the comparison consists of various statistics:</p>

<ul>
<li>Number of samples</li>
<li>Number of bits required to encode the whole data</li>
<li>Average number of bits per sample</li>
<li>Number of common bits shared by both data sources (reference and predictions)</li>
<li>Average number of common bits per sample shared by both data sources</li>
<li>Accuracy, i.e. percentage of common bits</li>
</ul>

<p>Example application:</p>

<div class="highlight highlight-source-shell"><pre>aifad -spec foo.spec -model mymodel.model -eval output.data <span class="pl-k">&lt;</span> input.data</pre></div>

<h3>
<a id="model-complexity" class="anchor" href="#model-complexity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model Complexity</h3>

<p>After learning, the model complexity is printed to <code>stderr</code>.  It is a
dimensionless measure that tells how many bits of input are needed in average
to produce one bit of output.  Note that this is completely unrelated to
the accuracy of the model!</p>

<h3>
<a id="random-data-generation" class="anchor" href="#random-data-generation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Random Data Generation</h3>

<p>For testing purposes it is often very helpful to generate random
data given some specification.  You might want to try this out with
the specification <code>large.ads</code> in the <code>examples</code>-directory of the
distribution.  As usual, you will have to use the <code>-c45</code> flag if you
want C4.5-data.</p>

<p>The following generates ten random samples on <code>stdout</code> for a standard AIFAD
specification <code>foo.ads</code>:</p>

<div class="highlight highlight-source-shell"><pre>aifad -rand-gen 10 -spec foo.ads</pre></div>

<p>If you do not want to generate data for the target variable(s), use this
instead:</p>

<div class="highlight highlight-source-shell"><pre>aifad -rand-gen-no-target 10 -spec foo.ads</pre></div>

<p>You can even simulate the presence of missing values by setting their
probability (default: 0.01):</p>

<div class="highlight highlight-source-shell"><pre>aifad -rand-gen 10 -rand-mv-prob 0.05 -spec foo.ads</pre></div>

<p>If you want to pass a certain seed to the random number generator, use
<code>-rand-init</code>.  By using <code>-rand-self-init</code>, it will be initialized at startup
in a system-dependent way.</p>

<hr>

<h2>
<a id="contact-information-and-contributing" class="anchor" href="#contact-information-and-contributing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact Information and Contributing</h2>

<p>Since there are hardly any real world data sets that exploit the advanced
data representation features provided by AIFAD, I would be very grateful to
hear about success stories, especially what concerns improved accuracy and
model complexity over less expressive representations.</p>

<p>In the case of bugs, feature requests, contributions and similar, you can
contact me here: <a href="mailto:markus.mottl@gmail.com">markus.mottl@gmail.com</a></p>

<p>Up-to-date information concerning this tool should be available at:
<a href="http://mmottl.github.io/aifad">http://mmottl.github.io/aifad</a></p>

<p>Enjoy!</p>

<p>Markus Mottl in Rutherford, NJ on July 01, 2014</p>

<p>[^lexical]: Some equivalent definitions copied from the OCaml-manual.</p>

<p>[^quinlan1986]: J. R. Quinlan.  Induction of decision trees.  In Jude
W. Shavlik and Thomas G. Dietterich, editors, <em>Readings in Machine Learning</em>.
Morgan Kaufmann, 1990.  Originally published in <em>Machine Learning</em> 1:81-106,
1986.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/mmottl/aifad">AIFAD</a> is maintained by <a href="https://github.com/mmottl">mmottl</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
