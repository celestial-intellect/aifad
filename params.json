{
  "name": "AIFAD",
  "tagline": "AIFAD - Automated Induction of Functions over Algebraic Data Types is a machine learning library and application written in OCaml which generalizes decision tree learning to algebraic data types.",
  "body": "AIFAD - Automated Induction of Functions over Algebraic Data Types\r\n==================================================================\r\n\r\n---------------------------------------------------------------------------\r\n\r\nWhat is AIFAD?\r\n--------------\r\n\r\nAIFAD stands for _Automated Induction of Functions over Algebraic Data Types_\r\nand is an application written in [OCaml](http://www.ocaml.org) that improves\r\ndecision tree learning by supporting significantly more complex kinds of data.\r\nThis allows users to more conveniently describe the data they want to learn\r\nfunctions on and can improve the accuracy and complexity of resulting models.\r\n\r\nFeatures\r\n--------\r\n\r\n  * Handles multi-valued attributes.  This has already become widespread among\r\n    decision tree learners, but some implementations still only support\r\n    binary ones.\r\n\r\n  * Allows multiple target attributes.  There are hardly any implementations\r\n    that support this feature in a unified way.\r\n\r\n  * Supports structured data, both for input and target attributes.\r\n    Structured data is still a hot research topic in machine learning.\r\n\r\n  * Allows recursive data.  AIFAD can learn models for recursive data but\r\n    only induce non-recursive functions.\r\n\r\n  * Handles missing values by allowing users to encode them as option types.\r\n    This is a clean solution compared to often-employed hacks and has\r\n    competitive accuracy.\r\n\r\n  * Provides several heuristics for generating models, including a generalized\r\n    gain ratio criterion as used by the state-of-the-art decision tree\r\n    learner C4.5.\r\n\r\n  * Can read data specifications as understood by C4.5.\r\n\r\n  * Pretty efficient.  Even though it was written to handle structured\r\n    data, it is only about 3-5 times slower than C4.5 on datasets that the\r\n    latter supports.  The memory footprint is somewhat larger due to the\r\n    impossibility of performing certain operations in-place, but still low\r\n    enough to handle even large datasets (hundreds of thousands of samples\r\n    with 10s of attributes) on stock hardware.\r\n\r\nAIFAD essentially offers you the same amount of expressiveness on both sides\r\nof the data (input and output).  Discrete data specifications as supported\r\nby more common decision tree learners (e.g. C4.5) are a strict subcase of\r\nalgebraic data types as used in AIFAD.  Furthermore, all kinds of algorithms\r\nthat operate on \"normal\" decision trees (e.g. pruning algorithms) should be\r\ngeneralizable to the more expressive representation.\r\n\r\n### Missing Features\r\n\r\n* Cannot (yet) handle numerical values.  Users can work around this by\r\n  preprocessing numeric data to cast it into discrete domains.  Since AIFAD\r\n  supports structured data, encodings more flexible than mere assignment to\r\n  \"flat\" domains can be used.  This will likely improve predictive accuracy.\r\n  If there is numerical data in C4.5-data, AIFAD will ignore it during\r\n  learning, thus only considering discrete data.\r\n\r\n* Does not (yet) provide post-pruning.  Can be easily added (e.g. reduced\r\n  error pruning).\r\n\r\n* Cannot (yet) learn recursive functions.\r\n\r\n---------------------------------------------------------------------------\r\n\r\nUsing AIFAD\r\n-----------\r\n\r\n### Specification of Algebraic Data Types\r\n\r\nBefore AIFAD can learn from data, you will have to tell it what your data\r\nlooks like.  This requires creating a file with the extension `.ads`\r\n(algebraic data type specification), which contains a set of (possibly\r\nrecursive) type equations.  If you happen to know modern functional or logic\r\nprogramming languages, this concept will be common to you.\r\n\r\nHere is an example of a specification file (file `examples/test.ads` in\r\nthe distribution):\r\n\r\n```text\r\n# This is a comment\r\n\r\ndiameter = Large | Small.\r\n\r\ncheese = Mozzarella | Gorgonzola.\r\n\r\ntopping = Cheese cheese | Tomatoes | Mushrooms.\r\n\r\nspiced = False | True.\r\n\r\nmeal =\r\n  | WienerSchnitzel diameter\r\n  | Pizza (diameter * spiced * topping)\r\n  | TapirSoup spiced.\r\n\r\ndrink = Water | Beer | Wine.\r\n\r\nsatisfaction = Low | High | Very satisfaction.\r\n\r\ndomain = meal.\r\ncodomain = satisfaction.\r\n```\r\n\r\nThe above equations contain the name of data types on their left-hand side\r\nand their specification on the right-hand side.  For example, a value of\r\ntype `cheese` can be `Mozzarella` or `Gorgonzola`.  A (pizza) topping can\r\nbe `Cheese`, `Tomatoes`, or `Mushrooms`, but `Cheese` values require the\r\nparticular sort of cheese, e.g. `Cheese Gorgonzola`.\r\n\r\nSome kinds of data can also be described in more detail by several other data\r\nvalues.  For example, the `meal` value `Pizza` requires the specification of\r\n`diameter`, `spiced`, and `topping`.  An example instance of this type of\r\nvalue is:\r\n\r\n```text\r\nPizza (Large, False, Cheese Mozzarella)\r\n```\r\n\r\nThe data type `satisfaction` demonstrates recursion.  A restaurant guest's\r\nsatisfaction might be `Low`, `High`, `Very High`, `Very (Very Low)`, etc.\r\n\r\n`domain` is a special typename and tells AIFAD what values should be used as\r\ninput to your problem, whereas `codomain` specifies the output.  This example\r\nobviously sets up the specification of how to predict a restaurant guest's\r\nsatisfaction given the meal.\r\n\r\nNote that if specifications do not make sense (e.g. if types cannot be reached,\r\nviolate liveness properties, etc.), then they are currently pruned down to the\r\nlargest set of type equations that is still reasonable.  AIFAD currently does\r\nnot yet provide good error messages to help users improve their specifications,\r\nbut for most applications this should not be much of a concern.\r\n\r\n#### Lexical conventions[^lexical]\r\n\r\nThe following characters are considered blanks: space, newline, horizontal\r\ntabulation, carriage return, line feed and form feed.  Blanks are ignored,\r\nbut they separate adjacent identifiers, literals and keywords that would\r\notherwise be confused as one single identifier, literal or keyword.\r\n\r\nComments in specifications start with a hash `#` and are valid up to the\r\nend of the current line.\r\n\r\nIdentifiers are sequences of letters, digits, \\_ (the underscore character),\r\nand ' (the single quote), starting with a letter or an underscore.  Letters\r\ncontain at least the 52 lowercase and uppercase letters from the ASCII set.\r\nThe current implementation (except on MacOS) also recognizes as letters all\r\naccented characters from the ISO 8859-1 (`ISO Latin 1`) set.  All characters\r\nin an identifier are meaningful.  The current implementation places no limits\r\non the number of characters of an identifier.\r\n\r\nType names are identifiers, but start with either a lowercase character or\r\nan underscore \\_.  Data constructors are identifiers, but always start with\r\nuppercase characters.  The only keywords are `domain` and `codomain`.\r\n\r\n#### Syntax\r\n\r\nHere is the EBNF-grammar of type definitions, of which you can have almost\r\narbitrarily many in your specification:\r\n\r\n```text\r\ntype-definition ::= type-name '=' rhs '.'\r\nrhs             ::= sum { '|' sum }* | type\r\nsum             ::= data-constructor | data-constructor type\r\ntype            ::= type-name | product\r\nproduct         ::= '(' type { '*' type }+ ')'\r\n```\r\n\r\nTypes can only be defined once.  Data constructors may appear only once at\r\neach right-hand side, but may be reused in other definitions.  You will have\r\nto name one of your types `domain`, which defines the shape of your input\r\ndata, and another type `codomain` to indicate the form of your output data.\r\n\r\n### Data Files\r\n\r\nData files take the extension `.add`.  Here is an example for a data file\r\nused for learning models (file `examples/test.add` in the distribution):\r\n\r\n```text\r\nWienerSchnitzel Large -> High.\r\nWienerSchnitzel Small -> Low.\r\nPizza (Large, True, Cheese Mozzarella) -> Very High.\r\nPizza (Large, False, Cheese Gorgonzola) -> Very (Very High).\r\nPizza (Small, True, Cheese Mozzarella) -> High.\r\nPizza (Small, False, Cheese Gorgonzola) -> Very High.\r\nTapirSoup False -> Very Low.\r\nTapirSoup True -> Very Low.\r\nTapirSoup True -> Very High.\r\n```\r\n\r\nSuch files follow the same lexical conventions as data specifications.\r\n\r\nThere are three kinds of data files:\r\n\r\n  1. Files containing mappings from input values to output values used for\r\n     training (see example above).\r\n  2. Data files containing only input values.\r\n  3. Data files containing only output values.\r\n\r\nDepending on the operation you want to perform, one or several of the above\r\nformats are valid.\r\n\r\n#### Syntax\r\n\r\nHere is the EBNF-grammar of samples as used in data files containing mappings\r\nfrom input to output values:\r\n\r\n```text\r\nsample        ::= data '->' data '.'\r\ndata          ::= sum-value | product-value\r\nsum-value     ::= data-constructor | data-constructor argument\r\nargument      ::= data-constructor\r\n                | '(' data-constructor argument ')'\r\n                | product-value\r\nproduct-value ::= '(' data { ',' data }+ ')'\r\n```\r\n\r\nFiles that contain only input or output data do not contain `samples` but\r\n`data` followed by a dot.\r\n\r\n### C4.5-compatible Data\r\n\r\nAIFAD can handle data in C4.5-format.  Data specifications of this kind\r\nare stored in files having the extension `.names` and related data in files\r\nhaving the extension `.data`.  You can find an example for this data format\r\nin the files `c45.names` and `c45.data` in the `examples` directory in the\r\ndistribution.\r\n\r\nThe `.names`-file contains in its first line a comma-separated list of class\r\nvalues terminated by a dot.  Each following line starts with the name of an\r\nattribute followed by a colon and again a comma-separated list of attribute\r\nvalues terminated by a dot.  Instead of this list, the keyword \"continuous\"\r\nmay be used to indicate numerical data.  C4.5 can also handle continuous\r\nattributes, which are currently ignored by AIFAD.  This feature may be added\r\nsome time in the future.  AIFAD will, however, make use of all of the discrete\r\ndata available.\r\n\r\nData files in C4.5-format consist of comma-separated lists of data stored\r\nlinewise, which must match the order and contents of the attributes\r\nspecified in the corresponding `.names`-file.  The class values are\r\ncontained in the last column of these comma-separated lists.\r\n\r\n### Learning from Data\r\n\r\nUsing AIFAD on the command-line, the learning mode is turned on by the\r\nflag `-learn`.  In this case you will also have to provide a name for the\r\nspecification of the data.  This can be done in two ways:\r\n\r\n  1. Using the `-spec` flag, which takes the filename of the specification\r\n     but does not set a name for the data file (default: `stdin`).\r\n\r\n  2. Using the `-stem` flag, which expects a filestem and sets both the name\r\n     of the specification and the data appropriately.  E.g. `-stem foo`\r\n     would set a specification file `foo.ads` and a data file `foo.add`\r\n     for standard AIFAD data.  When using C4.5 data, the names would be\r\n     `foo.names` and `foo.data` respectively.  C4.5 mode can be indicated\r\n     with the flag `-c45` on the command-line.\r\n\r\n#### Examples\r\n\r\nThe following command learns standard AIFAD data, the specification being\r\nin file `foo.ads` while the data is being read via `stdin` from file `bar.add`:\r\n\r\n```sh\r\naifad -learn -spec foo.ads < bar.add\r\n```\r\n\r\nThe next command learns specification `foo.ads` and data `foo.add` given\r\na stem:\r\n\r\n```sh\r\naifad -learn -stem foo\r\n```\r\n\r\nThe following command learns specification `foo.names` and `foo.data` given\r\na stem (C4.5-format):\r\n\r\n```sh\r\naifad -c45 -learn -stem foo\r\n```\r\n\r\nBy default the system will learn a model and print it in a human-readable\r\nformat to `stdout`.  The model complexity, i.e. the number of bits consumed\r\nfrom input data to output one bit of information, is printed to `stderr`.\r\nIf `-no-hmod` is specified, no human-readable model will be printed.\r\n\r\nIf you also want to store the learned model in a machine-readable format for\r\nlater use (application), then you will have to specify a name for the model\r\nfile using the `-model` flag, e.g.:\r\n\r\n```sh\r\naifad -c45 -learn -stem foo -model mymodel.c45model\r\n```\r\n\r\nNote that C4.5-models and ones for standard AIFAD specifications are not\r\ncompatible.  AIFAD makes sure that you do not accidentally apply some model\r\nto the wrong data.\r\n\r\n#### Models\r\n\r\nModels are printed directly in the OCaml-language.  They are represented by a\r\nfunction `model`, which maps the tuple (or single attribute) of input to the\r\ncorresponding output.  Variables that are unused are unnamed, i.e. written as\r\nunderscore `_`.  The data constructors are represented by so-called polymorphic\r\nvariants so that there is no need for an explicit data specification.\r\nThe human-readable model file should actually be perfectly valid OCaml.\r\n\r\nInput data is matched in match-clauses so as to discriminate between\r\ndifferent cases.  As soon as the result is clear to the learning algorithm,\r\nconstructors of the output specification will be used to construct a result.\r\nIf the result is only partially known, let-bindings will factor out the\r\nalready predictable part, and more deeply nested matches predict the rest.\r\n\r\nWhen no other actions are mentioned on the command-line except for the\r\nmachine-readable model, the model will be printed in human-readable form on\r\n`stdout`, e.g.:\r\n\r\n```sh\r\naifad -model mymodel.c45model\r\n```\r\n\r\n### Learning Parameters\r\n\r\n#### Variable selection\r\n\r\nCurrently there is only one hardly different alternative to using the gain\r\nratio criterion, namely the one used by Quinlan in C4.5.  This criterion can be\r\nturned on using the flag `-gain-c45`.  It incorporates an additional constraint\r\nthat selects also on basis of the unscaled gains.  See [^quinlan1986]\r\nfor details.\r\n\r\n#### Pre-pruning\r\n\r\nThere is a currently still rather crude but quite effective and efficient\r\npre-pruning technique, which is turned on with `-with-min-gr`.  It stops\r\ngrowing decision-trees during learning in a branch when the number of bits\r\nrequired to select a variable for splitting exceeds the number of bits gained\r\nby this step.  This effectively sets a minimum gain ratio at each selection.\r\n\r\n#### Different entropy measures\r\n\r\nYou can choose `-indep-entropy` when you assume that tuples (including ones\r\nin substructures) are independent of each other.  This is faster than the\r\ndefault, which assumes dependence.\r\n\r\nChoose `-shallow-entropy` if you want to compute the entropy without\r\ndescending into structures.  This is much faster than the default, which\r\ndoes so, especially for deeply nested data structures.\r\n\r\n#### Most frequent values\r\n\r\nWhen no more splitting is possible or wanted, the most frequent value has\r\nto be computed from the available data.  By setting `-indep-most-freq`,\r\nyou can choose to assume independence of tuple elements in the data, the\r\ndefault assumes dependence again.\r\n\r\n#### Splitting null branches\r\n\r\nWhen splitting data depending on some variable, it can happen that one branch\r\nof the tree does not have any data for further learning (i.e. it is a _null\r\nbranch_).  Normally, the most frequent value of the previous set is then taken\r\nas leaf.  By setting `-split-null`, the learning process may continue using\r\nthe remaining variables and last dataset.  Sometimes this hardly makes any\r\ndifference in efficiency, sometimes it can lead to extremely long learning\r\ntimes and excessive model size.\r\n\r\nThough the learning time is generally polynomial in the size of the\r\ndataset, this flag can make the time complexity almost exponential in the\r\nbeginning, even for flat data structures.  When using structured data, time\r\ncomplexity is currently indeed exponential then, though this problem might\r\nbe solved in a future release using very complicated models that employ CPS\r\n(continuation-passing style).\r\n\r\nAt the moment we do not have empirical evidence that this experimental feature\r\nsignificantly improves learning, but there is still some experimentation\r\nleft to be done.\r\n\r\n#### Random models\r\n\r\nThere is an experimental feature which allows you to generate additional\r\nrandom models that are constructed such that at each split a variable is\r\nchosen from a distribution built from the gain ratios.  This can be turned\r\non with the flag `-n-rand-gain`, which expects the number of additional\r\nrandom models to generate as argument.  Another parameter `-t-rand-gain`\r\ncan be used to set the total number of seconds (floating point) that may be\r\nspent learning in this case.  Whichever limit is reached first (time limit\r\nor number of models) will determine the end of learning.\r\n\r\nThe models are necessarily equally accurate in-sample, but they may differ\r\nin complexity: the latter is taken as decision criterion on which model to\r\nchoose (minimum complexity).  This seems quite interesting from an empirical\r\npoint of view, but needs further investigation.\r\n\r\n#### Handling missing values in C4.5-data\r\n\r\nSamples containing missing values in their codomain are ignored.  For input\r\ndata there are four strategies for handling missing values (indicated by\r\n`?` in the data):\r\n\r\n  1. Ignore samples containing missing values.  Turned on by `-mv-ignore`.\r\n     This is recommended if you are absolutely sure that missing values\r\n     cannot occur, because it improves efficiency of learning, model size\r\n     and efficiency of application.\r\n\r\n  2. Predict the most probable (frequent) value when a missing-value is present\r\n    in a sample.  Turned on by `-mv-mprob`.  When learning such models,\r\n    the algebraic encoding of input data is either `None` or `Some data`,\r\n    where data is a tuple of all variables in the C4.5-specification.\r\n\r\n  3. Add another constructor to encode missing values (\"flat representation\").\r\n     Turned on by `-mv-flat`.\r\n\r\n  4. Algebraically encode the data by lifting constructors such that they\r\n     are either `None` if some value is missing in an attribute or `Some\r\n     value` if it is present.  This is the default (`-mv-lift-all`).\r\n\r\n#### Factorization of models\r\n\r\nModels are always constructed in such a way that matches are only performed\r\nwhen some information (also from substructures) is really needed.  It can,\r\nhowever, happen that it is only determinable after learning some subtree that\r\nsome partial predictions can already be made before the match.  By default,\r\nmodels will also be simplified in this respect by using let-bindings that\r\nfactor out common information from subbranches.  You can prohibit this\r\nfeature by passing `-no-factor`.  There is hardly any performance penalty\r\nfor factorization so you will usually be happy with the default.\r\n\r\n### Applying Models\r\n\r\nIf you have a machine-readable model file, you can apply it to input data.\r\nThere is a certain degree of flexibility here: you can apply a model either\r\nto samples, i.e. mappings from input to output data, or to input data only\r\n- even mixed!  In the case of mappings the output part in the data will be\r\nignore.  The input is read from standard input, the resulting predictions of\r\nthe model application will be printed in the corresponding format (standard\r\nAIFAD or C4.5) to `stdout` or to a file whose name is passed to the flag\r\n`-pred`.  For example:\r\n\r\n```sh\r\naifad -apply -model mymodel.c45model < input.data -pred prediction.data\r\n```\r\n\r\nThere is no need here to specify the type of model (C4.5), because the latter\r\nknows what it is.\r\n\r\n### Evaluating Results\r\n\r\nTo evaluate the accuracy of some model application, we compare its result\r\nto reference data.  The file containing the reference data is specified by\r\n`-eval`, the data to be evaluated is read from `stdin`.  You will also\r\nhave to supply a data specification by either using `-spec` or `-stem`.\r\nThe result of the comparison consists of various statistics:\r\n\r\n  * Number of samples\r\n  * Number of bits required to encode the whole data\r\n  * Average number of bits per sample\r\n  * Number of common bits shared by both data sources (reference and predictions)\r\n  * Average number of common bits per sample shared by both data sources\r\n  * Accuracy, i.e. percentage of common bits\r\n\r\nExample application:\r\n\r\n```sh\r\naifad -spec foo.spec -model mymodel.model -eval output.data < input.data\r\n```\r\n\r\n### Model Complexity\r\n\r\nAfter learning, the model complexity is printed to `stderr`.  It is a\r\ndimensionless measure that tells how many bits of input are needed in average\r\nto produce one bit of output.  Note that this is completely unrelated to\r\nthe accuracy of the model!\r\n\r\n### Random Data Generation\r\n\r\nFor testing purposes it is often very helpful to generate random\r\ndata given some specification.  You might want to try this out with\r\nthe specification `large.ads` in the `examples`-directory of the\r\ndistribution.  As usual, you will have to use the `-c45` flag if you\r\nwant C4.5-data.\r\n\r\nThe following generates ten random samples on `stdout` for a standard AIFAD\r\nspecification `foo.ads`:\r\n\r\n```sh\r\naifad -rand-gen 10 -spec foo.ads\r\n```\r\n\r\nIf you do not want to generate data for the target variable(s), use this\r\ninstead:\r\n\r\n```sh\r\naifad -rand-gen-no-target 10 -spec foo.ads\r\n```\r\n\r\nYou can even simulate the presence of missing values by setting their\r\nprobability (default: 0.01):\r\n\r\n```sh\r\naifad -rand-gen 10 -rand-mv-prob 0.05 -spec foo.ads\r\n```\r\n\r\nIf you want to pass a certain seed to the random number generator, use\r\n`-rand-init`.  By using `-rand-self-init`, it will be initialized at startup\r\nin a system-dependent way.\r\n\r\n---------------------------------------------------------------------------\r\n\r\nContact Information and Contributing\r\n------------------------------------\r\n\r\nSince there are hardly any real world data sets that exploit the advanced\r\ndata representation features provided by AIFAD, I would be very grateful to\r\nhear about success stories, especially what concerns improved accuracy and\r\nmodel complexity over less expressive representations.\r\n\r\nIn the case of bugs, feature requests, contributions and similar, you can\r\ncontact me here: <markus.mottl@gmail.com>\r\n\r\nUp-to-date information concerning this tool should be available at:\r\n<http://mmottl.github.io/aifad>\r\n\r\nEnjoy!\r\n\r\nMarkus Mottl in Rutherford, NJ on July 01, 2014\r\n\r\n[^lexical]: Some equivalent definitions copied from the OCaml-manual.\r\n\r\n[^quinlan1986]: J. R. Quinlan.  Induction of decision trees.  In Jude\r\nW. Shavlik and Thomas G. Dietterich, editors, _Readings in Machine Learning_.\r\nMorgan Kaufmann, 1990.  Originally published in _Machine Learning_ 1:81-106,\r\n1986.\r\n",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}